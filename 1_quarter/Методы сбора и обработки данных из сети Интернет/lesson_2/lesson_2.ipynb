{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "import mysql_connector as connector\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['name', 'sum', 'linc', 'website'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{73: 'Ростов-на-Дону', 4: 'Москва', 14: 'Санкт-Петербург', 13: 'Новосибирск', 33: 'Екатеринбург', 12: 'Нижний Новгород', 55: 'Казань', 1090: 'Нурлат', 106: 'Челябинск', 17: 'Омск', 5: 'Самара', 173: 'Уфа', 42: 'Воронеж', 88: 'Владимир', 2528: 'Владимиро-Александровское', 1644: 'Владимир-Волынский'}\n",
      "\n",
      "\n",
      "Выбор города. \n",
      "Введите наименование из списка выше: Уфа\n",
      "https://www.superjob.ru/vacancy/search/?keywords=sql&geo%5Bt%5D%5B0%5D=173\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>sum</th>\n",
       "      <th>linc</th>\n",
       "      <th>website</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Программист (Lexema, C#)</td>\n",
       "      <td>от 30000</td>\n",
       "      <td>https://www.superjob.ru/vakansii/programmist-2...</td>\n",
       "      <td>https://www.superjob.ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Специалист по внедрению и сопровождению ИС</td>\n",
       "      <td>от 25000</td>\n",
       "      <td>https://www.superjob.ru/vakansii/specialist-po...</td>\n",
       "      <td>https://www.superjob.ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Инженер-программист Delphi</td>\n",
       "      <td>По договоренности</td>\n",
       "      <td>https://www.superjob.ru/vakansii/inzhener-prog...</td>\n",
       "      <td>https://www.superjob.ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Инженер-программист</td>\n",
       "      <td>от 20000 до 100000</td>\n",
       "      <td>https://www.superjob.ru/vakansii/inzhener-prog...</td>\n",
       "      <td>https://www.superjob.ru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         name                 sum  \\\n",
       "1                    Программист (Lexema, C#)            от 30000   \n",
       "2  Специалист по внедрению и сопровождению ИС            от 25000   \n",
       "3                  Инженер-программист Delphi   По договоренности   \n",
       "4                         Инженер-программист  от 20000 до 100000   \n",
       "\n",
       "                                                linc                  website  \n",
       "1  https://www.superjob.ru/vakansii/programmist-2...  https://www.superjob.ru  \n",
       "2  https://www.superjob.ru/vakansii/specialist-po...  https://www.superjob.ru  \n",
       "3  https://www.superjob.ru/vakansii/inzhener-prog...  https://www.superjob.ru  \n",
       "4  https://www.superjob.ru/vakansii/inzhener-prog...  https://www.superjob.ru  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Константа основного сайта\n",
    "def get_main_url():\n",
    "    return 'https://www.superjob.ru'\n",
    "\n",
    "# Функция получаем из строки города и пишет их БД\n",
    "def get_geo_and_save_db():\n",
    "    #user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    html = requests.get(f'{get_main_url()}/vacancy/search/?keywords=sql&geo%5Bt%5D%5B0%5D=73&geo%5Bt%5D%5B1%5D=4&geo%5Bt%5D%5B2%5D=14&geo%5Bt%5D%5B3%5D=13&geo%5Bt%5D%5B4%5D=33&geo%5Bt%5D%5B5%5D=12&geo%5Bt%5D%5B6%5D=55&geo%5Bt%5D%5B7%5D=106&geo%5Bt%5D%5B8%5D=17&geo%5Bt%5D%5B9%5D=5&geo%5Bt%5D%5B10%5D=173&geo%5Bt%5D%5B11%5D=1090&geo%5Bt%5D%5B12%5D=42&geo%5Bt%5D%5B13%5D=88&geo%5Bt%5D%5B14%5D=2528&geo%5Bt%5D%5B15%5D=1644').text#, headers = user_agent\n",
    "    conn = connector.conn()\n",
    "    geo(conn, html)\n",
    "\n",
    "# функция URL для поиска вакансий\n",
    "def get_url(**params):\n",
    "    keywords = \"\"\n",
    "    geo = \"\"\n",
    "    if params.get(\"keywords\") != None:\n",
    "        keywords = params[\"keywords\"]\n",
    "    if params.get(\"geo\") != None:\n",
    "        geo = f'&geo%5Bt%5D%5B0%5D={params[\"geo\"]}'\n",
    "        \n",
    "    return f'{get_main_url()}/vacancy/search/?keywords={keywords}{geo}'\n",
    "\n",
    "# функция получения массива URL с детализацией вакансии\n",
    "def  get_page_vakansii(html):\n",
    "    \n",
    "    vakancii = html.find_all('a', {'class':'_1QIBo'})\n",
    "\n",
    "    return [get_main_url() + v.get('href') for v in vakancii] \n",
    "\n",
    "# функция получения наименования вакансии со страницы вакансии\n",
    "def get_name_vakansi(url):\n",
    "    html = requests.get(url).text\n",
    "    html = bs(html, 'lxml')\n",
    "    name_vakansi = html.find_all('h1', {'class':'_3mfro'})\n",
    "    \n",
    "    return name_vakansi[0].text  \n",
    "\n",
    "#Получение реквизитов вакансии\n",
    "def get_data_vakansii(url):\n",
    "    html1 = requests.get(url).text\n",
    "    html1 = bs(html1, 'lxml')\n",
    "    name_vakansi = html1.find_all('h1', {'class':'_3mfro'})\n",
    "\n",
    "    sum_vakansi = html1.find_all('span', {'class':'_2Wp8I'})\n",
    "    sum_vakansi = re.findall('<span>([\\d ]+)<\\/span>', str(sum_vakansi[0]).replace('\\xa0',''))\n",
    "\n",
    "    #print(name_vakansi[0].text, sum_vakansi[0])\n",
    "\n",
    "    #df.loc[len(df)+1] = \n",
    "    \n",
    "    sum_vakansi = sum_vakansi if len(sum_vakansi) > 0 else 'По договоренности'\n",
    "    \n",
    "    if type(sum_vakansi) == list:\n",
    "        sum_vakansi = f'от {sum_vakansi[0]} до {sum_vakansi[1]}' if len(sum_vakansi) > 1 else f'от {sum_vakansi[0]}'\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    return [name_vakansi[0].text, sum_vakansi, url, get_main_url()]\n",
    "\n",
    "# функция получения городов из кэша\n",
    "def geo(conn, html = None, geo_name = None):\n",
    "    if html != None:\n",
    "        html = bs(html, 'lxml')\n",
    "        scripts = html.find_all('a', {'class':'bv_II'})\n",
    "        i = 0\n",
    "        cites = {}\n",
    "        for script in scripts:\n",
    "\n",
    "            href = script.get('href')\n",
    "            #print(f'{i} {script}')\n",
    "            if href == None:\n",
    "                continue\n",
    "\n",
    "            if href.find('/vacancy/search/',0) < 0:\n",
    "                continue\n",
    "            i +=1\n",
    "            #print(f'{i} {href}')\n",
    "            city = bs(str(script), 'lxml')\n",
    "            pattern = '>([А-Яа-я\\ \\-]+)<'\n",
    "            string = str(city.find_all('span', {'class':'_3mfro'}))\n",
    "            city = re.findall(pattern,string)\n",
    "\n",
    "            if len(city)>0:\n",
    "                arr_id = []\n",
    "                pattern = '=([\\d]+)'\n",
    "                ids = re.findall(pattern,str(script))\n",
    "                for id in ids:\n",
    "                    arr_id.append(id)\n",
    "                cites[city[0]] =  arr_id\n",
    "        #print(f'{cites}')\n",
    "        \n",
    "        last_ids = cites[list(cites)[-1]]\n",
    "\n",
    "        for k, v in cites.items():\n",
    "            id = list((i for i in last_ids if i not in cites[k]))[0]\n",
    "            last_ids = cites[k]    \n",
    "            #print(f'{k} {list(id)[0]}')\n",
    "            query = f'delete from net.geo where id = \"{id}\"'\n",
    "            connector.exec(conn, query, '')\n",
    "            connector.exec(conn, f'''\n",
    "            insert into net.geo (id, name)\n",
    "            select {id} as id, \"{k}\" as name\n",
    "            from DUAL\n",
    "            where not exists(select 1 from net.geo where id = {id})''','')\n",
    "    if geo_name != None:\n",
    "        where = '' if geo_name == '' else f'where name = \"{geo_name}\"'\n",
    "        cursor = connector.get_date(conn, f'select id, name from net.geo {where}', '')\n",
    "        return cursor\n",
    "        \n",
    "\n",
    "# функция получения следующей страницы\n",
    "def get_next_page(pagas, url):\n",
    "    isFind = False\n",
    "    for page in pages:\n",
    "        if page.get('href').find(\"onMap\",0) != -1:\n",
    "            continue\n",
    "        new_url = get_main_url()+page.get('href')\n",
    "        if str(url) == new_url:\n",
    "            isFind = True\n",
    "        if (str(url) != new_url and isFind):\n",
    "            return new_url\n",
    "        #print(page.text)\n",
    "        \n",
    "    return None\n",
    "\n",
    "#Патерн для поиска следующей страницы через re. Может позже пригодится\n",
    "#patern = 'span class=\"_3IDf-\">([\\d]|[Дд][а-яА-Я]+)'\n",
    "\n",
    "# Функция формирует массив страниц\n",
    "# используем ее для последующего обхода\n",
    "def get_pages(pages,url):\n",
    "\n",
    "    urls = [url]\n",
    "    while 1==1:\n",
    "\n",
    "        next_url = get_next_page(pages,urls[len(urls)-1])\n",
    "\n",
    "        if next_url not in urls:\n",
    "            urls.append(next_url)\n",
    "        else:\n",
    "            break\n",
    "    return urls\n",
    "\n",
    "#у нас пока мало городов для поиска, но мы стремимся к большему\n",
    "# выбирите город в котором необходимо искать вакансии\n",
    "conn = connector.conn()\n",
    "cities = dict(geo(conn, None, \"\"))\n",
    "\n",
    "print(cities)\n",
    "\n",
    "inp = input('''\n",
    "\n",
    "Выбор города. \n",
    "Введите наименование из списка выше: ''')\n",
    "inp\n",
    "\n",
    "#ищем в БД\n",
    "cities = dict(geo(conn, None, inp))\n",
    "\n",
    "\n",
    "#если mysql не подключен то закоментарить все с верху до текущей строки и в \n",
    "\n",
    "city = list(cities)[0]\n",
    "if len(cities) > 0:\n",
    "    #Запрос в sj\n",
    "    url = get_url(keywords = \"sql\", geo = city)#(вакансия , город)\n",
    "    html = requests.get(url).text\n",
    "    html = bs(html, 'lxml')\n",
    "\n",
    "    #в url у нас основная страница отражающая результат поиска а также ссылки на страницы остальных результатов поиска\n",
    "    #соберем сначала все страницы в массив , а далее уже пройдем по массиву и обработаем рузультиаты поиска\n",
    "    pages = html.find_all('a', {'class':'_3ze9n'})\n",
    "\n",
    "    for page_search in get_pages(pages,url):\n",
    "        print(page_search)\n",
    "        if page_search == None:\n",
    "            continue\n",
    "        html = requests.get(page_search).text\n",
    "        html = bs(html, 'lxml')\n",
    "        #обход страниц вакансий и запись их в датафрейм\n",
    "        for vacancy in get_page_vakansii(html):\n",
    "            df.loc[len(df)+1] = get_data_vakansii(vacancy)\n",
    "# выводим результат в dataframe\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
