{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществим предобработку данных с Твиттера, чтобы отчищенный данные в дальнейшем использовать для задачи классификации. Данный датасет содержит негативные (label = 1) и нейтральные (label = 0) высказывания.\n",
    "Для работы объединим train_df и test_df.\n",
    "\n",
    "Задания:\n",
    "\n",
    "1) Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape(). Всю предобработку делаем в новом столбце 'clean_tweet'\n",
    "\n",
    "2) Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    " - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    " - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "при применении функции необходимо использовать np.vectorize(function).\n",
    "\n",
    "3) Изменим регистр твитов на нижний с помощью .lower().\n",
    "\n",
    "4) Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "5) Заменим сокращения на их полные формы, используя short_word_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "6) Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "7) Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'.\n",
    "\n",
    "8) Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'.\n",
    "\n",
    "9) Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'.\n",
    "\n",
    "10) Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1]).\n",
    "\n",
    "11) Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "12) Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "\n",
    "13) Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "14) Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "15) Сохраним результат предобработки в pickle-файл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "apostrophe_dict = {\n",
    "\"ain't\": \"am not / are not\",\n",
    "\"aren't\": \"are not / am not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he had / he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he shall / he will\",\n",
    "\"he'll've\": \"he shall have / he will have\",\n",
    "\"he's\": \"he has / he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'd'y\": \"how do you\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how has / how is\",\n",
    "\"i'd\": \"I had / I would\",\n",
    "\"i'd've\": \"I would have\",\n",
    "\"i'll\": \"I shall / I will\",\n",
    "\"i'll've\": \"I shall have / I will have\",\n",
    "\"i'm\": \"I am\",\n",
    "\"i've\": \"I have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it had / it would\",\n",
    "\"it'd've\": \"it would have\",\n",
    "\"it'll\": \"it shall / it will\",\n",
    "\"it'll've\": \"it shall have / it will have\",\n",
    "\"it's\": \"it has / it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"mightn't've\": \"might not have\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"mustn't've\": \"must not have\",\n",
    "\"needn't\": \"need not\",\n",
    "\"needn't've\": \"need not have\",\n",
    "\"o'clock\": \"of the clock\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"oughtn't've\": \"ought not have\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"shan't've\": \"shall not have\",\n",
    "\"she'd\": \"she had / she would\",\n",
    "\"she'd've\": \"she would have\",\n",
    "\"she'll\": \"she shall / she will\",\n",
    "\"she'll've\": \"she shall have / she will have\",\n",
    "\"she's\": \"she has / she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"shouldn't've\": \"should not have\",\n",
    "\"so've\": \"so have\",\n",
    "\"so's\": \"so as / so is\",\n",
    "\"that'd\": \"that would / that had\",\n",
    "\"that'd've\": \"that would have\",\n",
    "\"that's\": \"that has / that is\",\n",
    "\"there'd\": \"there had / there would\",\n",
    "\"there'd've\": \"there would have\",\n",
    "\"there's\": \"there has / there is\",\n",
    "\"they'd\": \"they had / they would\",\n",
    "\"they'd've\": \"they would have\",\n",
    "\"they'll\": \"they shall / they will\",\n",
    "\"they'll've\": \"they shall have / they will have\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"to've\": \"to have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we had / we would\",\n",
    "\"we'd've\": \"we would have\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we'll've\": \"we will have\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what shall / what will\",\n",
    "\"what'll've\": \"what shall have / what will have\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what has / what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"when's\": \"when has / when is\",\n",
    "\"when've\": \"when have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where has / where is\",\n",
    "\"where've\": \"where have\",\n",
    "\"who'll\": \"who shall / who will\",\n",
    "\"who'll've\": \"who shall have / who will have\",\n",
    "\"who's\": \"who has / who is\",\n",
    "\"who've\": \"who have\",\n",
    "\"why's\": \"why has / why is\",\n",
    "\"why've\": \"why have\",\n",
    "\"will've\": \"will have\",\n",
    "\"won't\": \"will not\",\n",
    "\"won't've\": \"will not have\",\n",
    "\"would've\": \"would have\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"wouldn't've\": \"would not have\",\n",
    "\"y'all\": \"you all\",\n",
    "\"y'all'd\": \"you all would\",\n",
    "\"y'all'd've\": \"you all would have\",\n",
    "\"y'all're\": \"you all are\",\n",
    "\"y'all've\": \"you all have\",\n",
    "\"you'd\": \"you had / you would\",\n",
    "\"you'd've\": \"you would have\",\n",
    "\"you'll\": \"you shall / you will\",\n",
    "\"you'll've\": \"you shall have / you will have\",\n",
    "\"you're\": \"you are\",\n",
    "\"you've\": \"you have\"\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "short_word_dict = {\n",
    "\"121\": \"one to one\",\n",
    "\"a/s/l\": \"age, sex, location\",\n",
    "\"adn\": \"any day now\",\n",
    "\"afaik\": \"as far as I know\",\n",
    "\"afk\": \"away from keyboard\",\n",
    "\"aight\": \"alright\",\n",
    "\"alol\": \"actually laughing out loud\",\n",
    "\"b4\": \"before\",\n",
    "\"b4n\": \"bye for now\",\n",
    "\"bak\": \"back at the keyboard\",\n",
    "\"bf\": \"boyfriend\",\n",
    "\"bff\": \"best friends forever\",\n",
    "\"bfn\": \"bye for now\",\n",
    "\"bg\": \"big grin\",\n",
    "\"bta\": \"but then again\",\n",
    "\"btw\": \"by the way\",\n",
    "\"cid\": \"crying in disgrace\",\n",
    "\"cnp\": \"continued in my next post\",\n",
    "\"cp\": \"chat post\",\n",
    "\"cu\": \"see you\",\n",
    "\"cul\": \"see you later\",\n",
    "\"cul8r\": \"see you later\",\n",
    "\"cya\": \"bye\",\n",
    "\"cyo\": \"see you online\",\n",
    "\"dbau\": \"doing business as usual\",\n",
    "\"fud\": \"fear, uncertainty, and doubt\",\n",
    "\"fwiw\": \"for what it's worth\",\n",
    "\"fyi\": \"for your information\",\n",
    "\"g\": \"grin\",\n",
    "\"g2g\": \"got to go\",\n",
    "\"ga\": \"go ahead\",\n",
    "\"gal\": \"get a life\",\n",
    "\"gf\": \"girlfriend\",\n",
    "\"gfn\": \"gone for now\",\n",
    "\"gmbo\": \"giggling my butt off\",\n",
    "\"gmta\": \"great minds think alike\",\n",
    "\"h8\": \"hate\",\n",
    "\"hagn\": \"have a good night\",\n",
    "\"hdop\": \"help delete online predators\",\n",
    "\"hhis\": \"hanging head in shame\",\n",
    "\"iac\": \"in any case\",\n",
    "\"ianal\": \"I am not a lawyer\",\n",
    "\"ic\": \"I see\",\n",
    "\"idk\": \"I don't know\",\n",
    "\"imao\": \"in my arrogant opinion\",\n",
    "\"imnsho\": \"in my not so humble opinion\",\n",
    "\"imo\": \"in my opinion\",\n",
    "\"iow\": \"in other words\",\n",
    "\"ipn\": \"I’m posting naked\",\n",
    "\"irl\": \"in real life\",\n",
    "\"jk\": \"just kidding\",\n",
    "\"l8r\": \"later\",\n",
    "\"ld\": \"later, dude\",\n",
    "\"ldr\": \"long distance relationship\",\n",
    "\"llta\": \"lots and lots of thunderous applause\",\n",
    "\"lmao\": \"laugh my ass off\",\n",
    "\"lmirl\": \"let's meet in real life\",\n",
    "\"lol\": \"laugh out loud\",\n",
    "\"ltr\": \"longterm relationship\",\n",
    "\"lulab\": \"love you like a brother\",\n",
    "\"lulas\": \"love you like a sister\",\n",
    "\"luv\": \"love\",\n",
    "\"m/f\": \"male or female\",\n",
    "\"m8\": \"mate\",\n",
    "\"milf\": \"mother I would like to fuck\",\n",
    "\"oll\": \"online love\",\n",
    "\"omg\": \"oh my god\",\n",
    "\"otoh\": \"on the other hand\",\n",
    "\"pir\": \"parent in room\",\n",
    "\"ppl\": \"people\",\n",
    "\"r\": \"are\",\n",
    "\"rofl\": \"roll on the floor laughing\",\n",
    "\"rpg\": \"role playing games\",\n",
    "\"ru\": \"are you\",\n",
    "\"shid\": \"slaps head in disgust\",\n",
    "\"somy\": \"sick of me yet\",\n",
    "\"sot\": \"short of time\",\n",
    "\"thanx\": \"thanks\",\n",
    "\"thx\": \"thanks\",\n",
    "\"ttyl\": \"talk to you later\",\n",
    "\"u\": \"you\",\n",
    "\"ur\": \"you are\",\n",
    "\"uw\": \"you’re welcome\",\n",
    "\"wb\": \"welcome back\",\n",
    "\"wfm\": \"works for me\",\n",
    "\"wibni\": \"wouldn't it be nice if\",\n",
    "\"wtf\": \"what the fuck\",\n",
    "\"wtg\": \"way to go\",\n",
    "\"wtgp\": \"want to go private\",\n",
    "\"ym\": \"young man\",\n",
    "\"gr8\": \"great\"\n",
    "}\n",
    "\n",
    "\n",
    "emoticon_dict = {\n",
    "\":)\": \"happy\",\n",
    "\":‑)\": \"happy\",\n",
    "\":-]\": \"happy\",\n",
    "\":-3\": \"happy\",\n",
    "\":->\": \"happy\",\n",
    "\"8-)\": \"happy\",\n",
    "\":-}\": \"happy\",\n",
    "\":o)\": \"happy\",\n",
    "\":c)\": \"happy\",\n",
    "\":^)\": \"happy\",\n",
    "\"=]\": \"happy\",\n",
    "\"=)\": \"happy\",\n",
    "\"<3\": \"happy\",\n",
    "\":-(\": \"sad\",\n",
    "\":(\": \"sad\",\n",
    "\":c\": \"sad\",\n",
    "\":<\": \"sad\",\n",
    "\":[\": \"sad\",\n",
    "\">:[\": \"sad\",\n",
    "\":{\": \"sad\",\n",
    "\">:(\": \"sad\",\n",
    "\":-c\": \"sad\",\n",
    "\":-< \": \"sad\",\n",
    "\":-[\": \"sad\",\n",
    "\":-||\": \"sad\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "import os\n",
    "\n",
    "import html.parser    \n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   0       \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run                      \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2    bihday your majesty                                                                                                       \n",
       "3  #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦                                        \n",
       "4   factsguide: society now    #motivation                                                                                     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv('train_tweets.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  31963   \n",
       "1  31964   \n",
       "2  31965   \n",
       "3  31966   \n",
       "4  31967   \n",
       "\n",
       "                                                                                                                                            tweet  \n",
       "0  #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦                                                       \n",
       "1   @user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why                                             \n",
       "2  safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!                                                                          \n",
       "3  is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite  \n",
       "4    3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦                                                    "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('test_tweets.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0.0     \n",
       "1  2   0.0     \n",
       "2  3   0.0     \n",
       "3  4   0.0     \n",
       "4  5   0.0     \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run                      \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2    bihday your majesty                                                                                                       \n",
       "3  #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦                                        \n",
       "4   factsguide: society now    #motivation                                                                                     "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
    "combine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 49159 entries, 0 to 49158\n",
      "Data columns (total 3 columns):\n",
      "id       49159 non-null int64\n",
      "label    31962 non-null float64\n",
      "tweet    49159 non-null object\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(combine_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing:\n",
    "\n",
    "    def __init__(self, **param):\n",
    "        if 'file_name' in param:\n",
    "            self.file_name = param['file_name']\n",
    "            self.df_load = pd.read_csv(self.file_name)\n",
    "            self.df = ''\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    \n",
    "    def _replace(self, text, dictonary, patern):\n",
    "        results = re.findall(patern, text)\n",
    "        for item in results:\n",
    "            word = dictonary.get(item)\n",
    "            if word != None:\n",
    "                text = re.sub(item, word, text)\n",
    "        return text\n",
    "\n",
    "    def get_patern_emoji(self):\n",
    "        simbol = ['[', '\\\\', '^', '$', '.', '|', '?', '*', '+', '(', ')']\n",
    "\n",
    "        def __replace(s):\n",
    "            new = ''\n",
    "            for item in s:\n",
    "                if item in simbol:\n",
    "                    new += f'\\{item}'\n",
    "                else:\n",
    "                    new += item\n",
    "            return new\n",
    "\n",
    "        return '|'.join([f'({__replace(x)})' for x in emoticon_dict])\n",
    "    \n",
    "    def preproces(self):\n",
    "\n",
    "        df = self.df_load.copy()\n",
    "\n",
    "        df['tweet'] = df['tweet'].astype('str') \n",
    " \n",
    "        # 1. Заменим html-сущности (к примеру: &lt; &gt; &amp;). \"&lt;\" заменим на “<” и \"&amp;\" заменим на “&”)\"\"\". Сделаем это с помощью HTMLParser.unescape()\n",
    "\n",
    "        df['clean_tweet'] = html.parser.HTMLParser().unescape(df['tweet'])\n",
    "\n",
    "\n",
    "        # 2. Удалим @user из всех твитов с помощью паттерна \"@[\\w]*\". Для этого создадим функцию: \n",
    "        #  - для того, чтобы найти все вхождения паттерна в тексте, необходимо использовать re.findall(pattern, input_txt)\n",
    "        #  - для для замены @user на пробел, необходимо использовать re.sub()\n",
    "        # при применении функции необходимо использовать np.vectorize(function)\n",
    "\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub('@[\\w]*','', x))\n",
    "\n",
    "        # 3. Изменим регистр твитов на нижний с помощью .lower()\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].str.lower()\n",
    "\n",
    "        # 4. Заменим сокращения с апострофами (пример: ain't, can't) на пробел, используя apostrophe_dict. \n",
    "        # Для этого необходимо сделать функцию: для каждого слова в тексте проверить (for word in text.split()), \n",
    "        # если слово есть в словаре apostrophe_dict в качестве ключа (сокращенного слова), \n",
    "        # то заменить ключ на значение (полную версию слова).\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: self._replace(x, apostrophe_dict, \"[\\w]*'[\\w]*\"))\n",
    "\n",
    "        # 5. Заменим сокращения на их полные формы, используя short_word_dict. \n",
    "        # Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: self._replace(x, short_word_dict, \"[ ]\"))\n",
    "\n",
    "        # 6. Заменим эмотиконы (пример: \":)\" = \"happy\") на пробелы, используя emoticon_dict. \n",
    "        # Для этого воспользуемся функцией, используемой в предыдущем пункте.\n",
    "        patern = self.get_patern_emoji()\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: self._replace(x, emoticon_dict,patern))\n",
    "        #'|'.join(['('+x+')' for x in emoticon_dict])\n",
    "\n",
    "        # 7. Заменим пунктуацию на пробелы, используя re.sub() и паттерн r'[^\\w\\s]'\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^\\w\\s]',' ', x))\n",
    "\n",
    "        # 8. Заменим спец. символы на пробелы, используя re.sub() и паттерн r'[^a-zA-Z0-9]'\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ', x))\n",
    "\n",
    "        # 9. Заменим числа на пробелы, используя re.sub() и паттерн r'[^a-zA-Z]'\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z]',' ', x))\n",
    "\n",
    "        # 10. Удалим из текста слова длиной в 1 символ, используя ' '.join([w for w in x.split() if len(w)>1])\n",
    "\n",
    "        df['clean_tweet'] = df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
    "\n",
    "        # 11. Поделим твиты на токены с помощью nltk.tokenize.word_tokenize, создав новый столбец 'tweet_token'.\n",
    "\n",
    "        df['tweet_token'] = df['clean_tweet'].apply(lambda x: str(word_tokenize(x)))\n",
    "        df['tweet_token'] = df['tweet_token'].astype('str') \n",
    "\n",
    "        # 12. Удалим стоп-слова из токенов, используя nltk.corpus.stopwords. \n",
    "        # Создадим столбец 'tweet_token_filtered' без стоп-слов.\n",
    "        sw = set(stopwords.words(\"english\"))\n",
    "        df['tweet_token_filtered'] = df['tweet_token'].apply(lambda x: str(list(set(eval(x)).difference(sw))))\n",
    "        # \n",
    "\n",
    "        # 13. Применим стемминг к токенам с помощью nltk.stem.PorterStemmer. \n",
    "        # Создадим столбец 'tweet_stemmed' после применения стемминга.\n",
    "\n",
    "        porter = PorterStemmer()\n",
    "        df['tweet_stemmed'] = df['tweet_token_filtered'].apply(lambda x: str([porter.stem(i) for i in eval(x)]))\n",
    "\n",
    "        # 14. Применим лемматизацию к токенам с помощью nltk.stem.wordnet.WordNetLemmatizer. \n",
    "        # Создадим столбец 'tweet_lemmatized' после применения лемматизации.\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        df['tweet_lemmatized'] = df['tweet_token_filtered'\n",
    "                                   ].apply(lambda x: str([lemmatizer.lemmatize(i,wordnet.VERB\n",
    "                                                                              ) for i in eval(x)]\n",
    "                                                        )\n",
    "                                          )\n",
    "        self.df = df\n",
    "    # 15. Сохраним результат предобработки в pickle-файл.        \n",
    "    def _save(self, path = ''):\n",
    "        self.df.to_pickle(f'{path}{self.file_name}.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем тренировочный файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   0       \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run                      \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2    bihday your majesty                                                                                                       \n",
       "3  #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦                                        \n",
       "4   factsguide: society now    #motivation                                                                                     "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = preprocessing(file_name = 'train_tweets.csv')\n",
    "pr.df_load.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "      <td>when father is dysfunctional and is so selfish he drags his kids into his dysfunction run</td>\n",
       "      <td>['when', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', 'run']</td>\n",
       "      <td>['run', 'father', 'selfish', 'kids', 'dysfunctional', 'dysfunction', 'drags']</td>\n",
       "      <td>['run', 'father', 'selfish', 'kid', 'dysfunct', 'dysfunct', 'drag']</td>\n",
       "      <td>['run', 'father', 'selfish', 'kid', 'dysfunctional', 'dysfunction', 'drag']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "      <td>thanks for lyft credit cannot use cause they do not offer wheelchair vans in pdx disapointed getthanked</td>\n",
       "      <td>['thanks', 'for', 'lyft', 'credit', 'can', 'not', 'use', 'cause', 'they', 'do', 'not', 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed', 'getthanked']</td>\n",
       "      <td>['lyft', 'offer', 'wheelchair', 'vans', 'cause', 'thanks', 'getthanked', 'pdx', 'use', 'disapointed', 'credit']</td>\n",
       "      <td>['lyft', 'offer', 'wheelchair', 'van', 'caus', 'thank', 'getthank', 'pdx', 'use', 'disapoint', 'credit']</td>\n",
       "      <td>['lyft', 'offer', 'wheelchair', 'vans', 'cause', 'thank', 'getthanked', 'pdx', 'use', 'disapointed', 'credit']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>bihday your majesty</td>\n",
       "      <td>['bihday', 'your', 'majesty']</td>\n",
       "      <td>['majesty', 'bihday']</td>\n",
       "      <td>['majesti', 'bihday']</td>\n",
       "      <td>['majesty', 'bihday']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "      <td>model love take with all the time in ur</td>\n",
       "      <td>['model', 'love', 'take', 'with', 'all', 'the', 'time', 'in', 'ur']</td>\n",
       "      <td>['model', 'ur', 'take', 'time', 'love']</td>\n",
       "      <td>['model', 'ur', 'take', 'time', 'love']</td>\n",
       "      <td>['model', 'ur', 'take', 'time', 'love']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "      <td>factsguide society now motivation</td>\n",
       "      <td>['factsguide', 'society', 'now', 'motivation']</td>\n",
       "      <td>['factsguide', 'motivation', 'society']</td>\n",
       "      <td>['factsguid', 'motiv', 'societi']</td>\n",
       "      <td>['factsguide', 'motivation', 'society']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0  1   0       \n",
       "1  2   0       \n",
       "2  3   0       \n",
       "3  4   0       \n",
       "4  5   0       \n",
       "\n",
       "                                                                                                                        tweet  \\\n",
       "0   @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run                       \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked   \n",
       "2    bihday your majesty                                                                                                        \n",
       "3  #model   i love u take with u all the time in urð±!!! ðððð\n",
       "ð¦ð¦ð¦                                         \n",
       "4   factsguide: society now    #motivation                                                                                      \n",
       "\n",
       "                                                                                               clean_tweet  \\\n",
       "0  when father is dysfunctional and is so selfish he drags his kids into his dysfunction run                 \n",
       "1  thanks for lyft credit cannot use cause they do not offer wheelchair vans in pdx disapointed getthanked   \n",
       "2  bihday your majesty                                                                                       \n",
       "3  model love take with all the time in ur                                                                   \n",
       "4  factsguide society now motivation                                                                         \n",
       "\n",
       "                                                                                                                                                       tweet_token  \\\n",
       "0  ['when', 'father', 'is', 'dysfunctional', 'and', 'is', 'so', 'selfish', 'he', 'drags', 'his', 'kids', 'into', 'his', 'dysfunction', 'run']                        \n",
       "1  ['thanks', 'for', 'lyft', 'credit', 'can', 'not', 'use', 'cause', 'they', 'do', 'not', 'offer', 'wheelchair', 'vans', 'in', 'pdx', 'disapointed', 'getthanked']   \n",
       "2  ['bihday', 'your', 'majesty']                                                                                                                                     \n",
       "3  ['model', 'love', 'take', 'with', 'all', 'the', 'time', 'in', 'ur']                                                                                               \n",
       "4  ['factsguide', 'society', 'now', 'motivation']                                                                                                                    \n",
       "\n",
       "                                                                                              tweet_token_filtered  \\\n",
       "0  ['run', 'father', 'selfish', 'kids', 'dysfunctional', 'dysfunction', 'drags']                                     \n",
       "1  ['lyft', 'offer', 'wheelchair', 'vans', 'cause', 'thanks', 'getthanked', 'pdx', 'use', 'disapointed', 'credit']   \n",
       "2  ['majesty', 'bihday']                                                                                             \n",
       "3  ['model', 'ur', 'take', 'time', 'love']                                                                           \n",
       "4  ['factsguide', 'motivation', 'society']                                                                           \n",
       "\n",
       "                                                                                              tweet_stemmed  \\\n",
       "0  ['run', 'father', 'selfish', 'kid', 'dysfunct', 'dysfunct', 'drag']                                        \n",
       "1  ['lyft', 'offer', 'wheelchair', 'van', 'caus', 'thank', 'getthank', 'pdx', 'use', 'disapoint', 'credit']   \n",
       "2  ['majesti', 'bihday']                                                                                      \n",
       "3  ['model', 'ur', 'take', 'time', 'love']                                                                    \n",
       "4  ['factsguid', 'motiv', 'societi']                                                                          \n",
       "\n",
       "                                                                                                 tweet_lemmatized  \n",
       "0  ['run', 'father', 'selfish', 'kid', 'dysfunctional', 'dysfunction', 'drag']                                     \n",
       "1  ['lyft', 'offer', 'wheelchair', 'vans', 'cause', 'thank', 'getthanked', 'pdx', 'use', 'disapointed', 'credit']  \n",
       "2  ['majesty', 'bihday']                                                                                           \n",
       "3  ['model', 'ur', 'take', 'time', 'love']                                                                         \n",
       "4  ['factsguide', 'motivation', 'society']                                                                         "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.preproces()\n",
    "pr._save()\n",
    "pr.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загружаем тестовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>clean_tweet</th>\n",
       "      <th>tweet_token</th>\n",
       "      <th>tweet_token_filtered</th>\n",
       "      <th>tweet_stemmed</th>\n",
       "      <th>tweet_lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦</td>\n",
       "      <td>studiolife aislife requires passion dedication willpower to find newmaterials</td>\n",
       "      <td>['studiolife', 'aislife', 'requires', 'passion', 'dedication', 'willpower', 'to', 'find', 'newmaterials']</td>\n",
       "      <td>['requires', 'aislife', 'passion', 'newmaterials', 'find', 'willpower', 'studiolife', 'dedication']</td>\n",
       "      <td>['requir', 'aislif', 'passion', 'newmateri', 'find', 'willpow', 'studiolif', 'dedic']</td>\n",
       "      <td>['require', 'aislife', 'passion', 'newmaterials', 'find', 'willpower', 'studiolife', 'dedication']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why</td>\n",
       "      <td>white supremacists want everyone to see the new birds movie and here why</td>\n",
       "      <td>['white', 'supremacists', 'want', 'everyone', 'to', 'see', 'the', 'new', 'birds', 'movie', 'and', 'here', 'why']</td>\n",
       "      <td>['birds', 'everyone', 'white', 'see', 'want', 'movie', 'new', 'supremacists']</td>\n",
       "      <td>['bird', 'everyon', 'white', 'see', 'want', 'movi', 'new', 'supremacist']</td>\n",
       "      <td>['bird', 'everyone', 'white', 'see', 'want', 'movie', 'new', 'supremacists']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!</td>\n",
       "      <td>safe ways to heal your acne altwaystoheal healthy healing</td>\n",
       "      <td>['safe', 'ways', 'to', 'heal', 'your', 'acne', 'altwaystoheal', 'healthy', 'healing']</td>\n",
       "      <td>['ways', 'healing', 'safe', 'healthy', 'altwaystoheal', 'heal', 'acne']</td>\n",
       "      <td>['way', 'heal', 'safe', 'healthi', 'altwaystoh', 'heal', 'acn']</td>\n",
       "      <td>['ways', 'heal', 'safe', 'healthy', 'altwaystoheal', 'heal', 'acne']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite</td>\n",
       "      <td>is the hp and the cursed child book up for reservations already if yes where if no when harrypotter pottermore favorite</td>\n",
       "      <td>['is', 'the', 'hp', 'and', 'the', 'cursed', 'child', 'book', 'up', 'for', 'reservations', 'already', 'if', 'yes', 'where', 'if', 'no', 'when', 'harrypotter', 'pottermore', 'favorite']</td>\n",
       "      <td>['hp', 'already', 'book', 'cursed', 'child', 'pottermore', 'reservations', 'favorite', 'harrypotter', 'yes']</td>\n",
       "      <td>['hp', 'alreadi', 'book', 'curs', 'child', 'pottermor', 'reserv', 'favorit', 'harrypott', 'ye']</td>\n",
       "      <td>['hp', 'already', 'book', 'curse', 'child', 'pottermore', 'reservations', 'favorite', 'harrypotter', 'yes']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦</td>\n",
       "      <td>rd bihday to my amazing hilarious nephew eli ahmir uncle dave loves you and misses</td>\n",
       "      <td>['rd', 'bihday', 'to', 'my', 'amazing', 'hilarious', 'nephew', 'eli', 'ahmir', 'uncle', 'dave', 'loves', 'you', 'and', 'misses']</td>\n",
       "      <td>['hilarious', 'uncle', 'ahmir', 'nephew', 'dave', 'rd', 'misses', 'amazing', 'loves', 'bihday', 'eli']</td>\n",
       "      <td>['hilari', 'uncl', 'ahmir', 'nephew', 'dave', 'rd', 'miss', 'amaz', 'love', 'bihday', 'eli']</td>\n",
       "      <td>['hilarious', 'uncle', 'ahmir', 'nephew', 'dave', 'rd', 'miss', 'amaze', 'love', 'bihday', 'eli']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  \\\n",
       "0  31963   \n",
       "1  31964   \n",
       "2  31965   \n",
       "3  31966   \n",
       "4  31967   \n",
       "\n",
       "                                                                                                                                            tweet  \\\n",
       "0  #studiolife #aislife #requires #passion #dedication #willpower   to find #newmaterialsâ¦                                                        \n",
       "1   @user #white #supremacists want everyone to see the new â  #birdsâ #movie â and hereâs why                                              \n",
       "2  safe ways to heal your #acne!!    #altwaystoheal #healthy   #healing!!                                                                           \n",
       "3  is the hp and the cursed child book up for reservations already? if yes, where? if no, when? ððð   #harrypotter #pottermore #favorite   \n",
       "4    3rd #bihday to my amazing, hilarious #nephew eli ahmir! uncle dave loves you and missesâ¦                                                     \n",
       "\n",
       "                                                                                                               clean_tweet  \\\n",
       "0  studiolife aislife requires passion dedication willpower to find newmaterials                                             \n",
       "1  white supremacists want everyone to see the new birds movie and here why                                                  \n",
       "2  safe ways to heal your acne altwaystoheal healthy healing                                                                 \n",
       "3  is the hp and the cursed child book up for reservations already if yes where if no when harrypotter pottermore favorite   \n",
       "4  rd bihday to my amazing hilarious nephew eli ahmir uncle dave loves you and misses                                        \n",
       "\n",
       "                                                                                                                                                                               tweet_token  \\\n",
       "0  ['studiolife', 'aislife', 'requires', 'passion', 'dedication', 'willpower', 'to', 'find', 'newmaterials']                                                                                 \n",
       "1  ['white', 'supremacists', 'want', 'everyone', 'to', 'see', 'the', 'new', 'birds', 'movie', 'and', 'here', 'why']                                                                          \n",
       "2  ['safe', 'ways', 'to', 'heal', 'your', 'acne', 'altwaystoheal', 'healthy', 'healing']                                                                                                     \n",
       "3  ['is', 'the', 'hp', 'and', 'the', 'cursed', 'child', 'book', 'up', 'for', 'reservations', 'already', 'if', 'yes', 'where', 'if', 'no', 'when', 'harrypotter', 'pottermore', 'favorite']   \n",
       "4  ['rd', 'bihday', 'to', 'my', 'amazing', 'hilarious', 'nephew', 'eli', 'ahmir', 'uncle', 'dave', 'loves', 'you', 'and', 'misses']                                                          \n",
       "\n",
       "                                                                                           tweet_token_filtered  \\\n",
       "0  ['requires', 'aislife', 'passion', 'newmaterials', 'find', 'willpower', 'studiolife', 'dedication']            \n",
       "1  ['birds', 'everyone', 'white', 'see', 'want', 'movie', 'new', 'supremacists']                                  \n",
       "2  ['ways', 'healing', 'safe', 'healthy', 'altwaystoheal', 'heal', 'acne']                                        \n",
       "3  ['hp', 'already', 'book', 'cursed', 'child', 'pottermore', 'reservations', 'favorite', 'harrypotter', 'yes']   \n",
       "4  ['hilarious', 'uncle', 'ahmir', 'nephew', 'dave', 'rd', 'misses', 'amazing', 'loves', 'bihday', 'eli']         \n",
       "\n",
       "                                                                                     tweet_stemmed  \\\n",
       "0  ['requir', 'aislif', 'passion', 'newmateri', 'find', 'willpow', 'studiolif', 'dedic']             \n",
       "1  ['bird', 'everyon', 'white', 'see', 'want', 'movi', 'new', 'supremacist']                         \n",
       "2  ['way', 'heal', 'safe', 'healthi', 'altwaystoh', 'heal', 'acn']                                   \n",
       "3  ['hp', 'alreadi', 'book', 'curs', 'child', 'pottermor', 'reserv', 'favorit', 'harrypott', 'ye']   \n",
       "4  ['hilari', 'uncl', 'ahmir', 'nephew', 'dave', 'rd', 'miss', 'amaz', 'love', 'bihday', 'eli']      \n",
       "\n",
       "                                                                                              tweet_lemmatized  \n",
       "0  ['require', 'aislife', 'passion', 'newmaterials', 'find', 'willpower', 'studiolife', 'dedication']           \n",
       "1  ['bird', 'everyone', 'white', 'see', 'want', 'movie', 'new', 'supremacists']                                 \n",
       "2  ['ways', 'heal', 'safe', 'healthy', 'altwaystoheal', 'heal', 'acne']                                         \n",
       "3  ['hp', 'already', 'book', 'curse', 'child', 'pottermore', 'reservations', 'favorite', 'harrypotter', 'yes']  \n",
       "4  ['hilarious', 'uncle', 'ahmir', 'nephew', 'dave', 'rd', 'miss', 'amaze', 'love', 'bihday', 'eli']            "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr = preprocessing(file_name = 'test_tweets.csv')\n",
    "pr.preproces()\n",
    "pr._save()\n",
    "pr.df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install en_core_web_md\n",
    "# !python -m spacy download en_core_web_sm\n",
    "#from spacy import displacy#библиолтека для визуализации текста и NER\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# displacy.render(doc, jupyter=True, style='ent')\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "def get_label(text):\n",
    "    global df\n",
    "    doc  = nlp(text)\n",
    "    for entity in doc.ents:\n",
    "        df = df.append({'Type':entity.label_, 'Value':entity.text}, ignore_index=True)\n",
    "    return doc.ents[0].label_ if len(doc.ents)>0 else None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr.df['tweet_ner'] = pr.df['clean_tweet'].apply(lambda x: get_label(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_ner</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DATE</td>\n",
       "      <td>2760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ORG</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPE</td>\n",
       "      <td>888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>TIME</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NORP</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CARDINAL</td>\n",
       "      <td>274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ORDINAL</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FAC</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LOC</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>EVENT</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>PRODUCT</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LANGUAGE</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>QUANTITY</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>WORK_OF_ART</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MONEY</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LAW</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>PERCENT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_ner  counts\n",
       "1   DATE         2760  \n",
       "13  PERSON       2006  \n",
       "11  ORG          1509  \n",
       "4   GPE          888   \n",
       "16  TIME         486   \n",
       "9   NORP         326   \n",
       "0   CARDINAL     274   \n",
       "10  ORDINAL      167   \n",
       "3   FAC          61    \n",
       "7   LOC          40    \n",
       "2   EVENT        34    \n",
       "14  PRODUCT      9     \n",
       "5   LANGUAGE     6     \n",
       "15  QUANTITY     6     \n",
       "17  WORK_OF_ART  5     \n",
       "8   MONEY        4     \n",
       "6   LAW          3     \n",
       "12  PERCENT      1     "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.df.groupby(['tweet_ner']).size().reset_index(name='counts'\n",
    "                                               ).sort_values(by=['counts'], \n",
    "                                                             ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_top = df.loc[(df['Type']=='ORG')|(df['Type']=='PERSON')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Value</th>\n",
       "      <th>Type</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1384</th>\n",
       "      <td>hea</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>522</th>\n",
       "      <td>bong bing</td>\n",
       "      <td>ORG</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1472</th>\n",
       "      <td>hu</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>app</td>\n",
       "      <td>ORG</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>gop</td>\n",
       "      <td>ORG</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1086</th>\n",
       "      <td>feminismiscancer feminismisterrorism feminismmuktbharat</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1559</th>\n",
       "      <td>islam</td>\n",
       "      <td>ORG</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>bihday</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>sta</td>\n",
       "      <td>ORG</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>amazon</td>\n",
       "      <td>ORG</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>christina grimmie</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>blur sun</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>house</td>\n",
       "      <td>ORG</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>clinton</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1428</th>\n",
       "      <td>hillary</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2049</th>\n",
       "      <td>lt lt lt lt</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2329</th>\n",
       "      <td>nba</td>\n",
       "      <td>ORG</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3328</th>\n",
       "      <td>usa</td>\n",
       "      <td>ORG</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2735</th>\n",
       "      <td>regrann</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1069</th>\n",
       "      <td>fbi</td>\n",
       "      <td>ORG</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                        Value    Type  counts\n",
       "1384  hea                                                      PERSON  35    \n",
       "522   bong bing                                                ORG     32    \n",
       "1472  hu                                                       PERSON  26    \n",
       "212   app                                                      ORG     24    \n",
       "1259  gop                                                      ORG     24    \n",
       "1086  feminismiscancer feminismisterrorism feminismmuktbharat  PERSON  20    \n",
       "1559  islam                                                    ORG     18    \n",
       "387   bihday                                                   PERSON  17    \n",
       "3037  sta                                                      ORG     16    \n",
       "133   amazon                                                   ORG     14    \n",
       "685   christina grimmie                                        PERSON  14    \n",
       "500   blur sun                                                 PERSON  12    \n",
       "1467  house                                                    ORG     12    \n",
       "709   clinton                                                  PERSON  11    \n",
       "1428  hillary                                                  PERSON  11    \n",
       "2049  lt lt lt lt                                              PERSON  11    \n",
       "2329  nba                                                      ORG     11    \n",
       "3328  usa                                                      ORG     10    \n",
       "2735  regrann                                                  PERSON  10    \n",
       "1069  fbi                                                      ORG     9     "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top = df_top.groupby(['Value','Type']).size().reset_index(name='counts').sort_values(by=['counts'], \n",
    "                                                             ascending=False).head(20)\n",
    "df_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nltk = pd.DataFrame()\n",
    "\n",
    "def get_label_nltk(text):\n",
    "    global df_nltk\n",
    "    text = f'{text[0].upper()}{text[1:]}'\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                df_nltk = df_nltk.append({'Type':chunk.label(), 'Value':chunk[0][0]}, ignore_index=True)\n",
    "                return chunk.label()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Studiolife</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPE</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Safe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Rd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Finished</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6520</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6521</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6522</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Damn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>GPE</td>\n",
       "      <td>Thought</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>Hillary</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6525 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Type       Value\n",
       "0     GPE     Studiolife\n",
       "1     GPE     White     \n",
       "2     GPE     Safe      \n",
       "3     GPE     Rd        \n",
       "4     GPE     Finished  \n",
       "...   ...          ...  \n",
       "6520  GPE     Good      \n",
       "6521  GPE     Black     \n",
       "6522  GPE     Damn      \n",
       "6523  GPE     Thought   \n",
       "6524  PERSON  Hillary   \n",
       "\n",
       "[6525 rows x 2 columns]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.df['tweet_ner_nltk'] = pr.df['clean_tweet'].apply(lambda x: get_label_nltk(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_ner_nltk</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GPE</td>\n",
       "      <td>5601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PERSON</td>\n",
       "      <td>649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GSP</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORGANIZATION</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FACILITY</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tweet_ner_nltk  counts\n",
       "1  GPE            5601  \n",
       "4  PERSON         649   \n",
       "2  GSP            22    \n",
       "3  ORGANIZATION   3     \n",
       "0  FACILITY       2     "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pr.df.groupby(['tweet_ner_nltk']).size().reset_index(name='counts'\n",
    "                                               ).sort_values(by=['counts'], \n",
    "                                                             ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3\n",
    "\n",
    "lib   | recognize NER | count\n",
    "\n",
    "spicy | 17            | почти все \n",
    "\n",
    "nltk  | 5             | мало\n",
    "\n",
    "nltk - во многих источниках пишут , что не производительная и библилтека с низким результатом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
